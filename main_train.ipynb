{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "#import torchvision.transforms as tt\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from utils.metrics import recall_k, ndcg_k\n",
    "from data.dataset_creation import DatasetInit, CustomDatasetSmall, CustomDatasetLarge, ToDevice\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "from scipy.sparse import coo_matrix\n",
    "import numpy as np\n",
    "\n",
    "import wandb\n",
    "import os\n",
    "os.environ[\"WANDB_API_KEY\"] = '0df87019b9f607855a97d62699a36b21b11eea04'\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import os\n",
    "\n",
    "def set_seed(seed: int = 42) -> None:\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    print(f\"Random seed set as {seed}\")\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set as 2007\n"
     ]
    }
   ],
   "source": [
    "set_seed(2007)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "history_len=20\n",
    "item_embed_size=128\n",
    "user_embed_size=32\n",
    "\n",
    "user_item_hidden_size = 128\n",
    "user_item_history_hidden_size = 128\n",
    "lstm_hidden_size = 128\n",
    "dense_1_hidden_size = 128\n",
    "dense_2_hidden_size = 128\n",
    "\n",
    "dataset_name = 'dunnhumby_cj' + 'test'\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-05-27 19:17:59--  https://raw.githubusercontent.com/mzhariann/recanet/main/data/dunnhumby_cj/train_baskets_sample.csv\n",
      "Распознаётся raw.githubusercontent.com (raw.githubusercontent.com)… 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Подключение к raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... соединение установлено.\n",
      "HTTP-запрос отправлен. Ожидание ответа… 200 OK\n",
      "Длина: 2706540 (2,6M) [text/plain]\n",
      "Сохранение в: «train_baskets_sample.csv»\n",
      "\n",
      "train_baskets_sampl 100%[===================>]   2,58M  1,55MB/s    за 1,7s    \n",
      "\n",
      "2023-05-27 19:18:02 (1,55 MB/s) - «train_baskets_sample.csv» сохранён [2706540/2706540]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/mzhariann/recanet/main/data/dunnhumby_cj/train_baskets_sample.csv\n",
    "!mv train_baskets_sample.csv data/dunnhumby_cj/train_baskets_sample.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = f'data/{dataset_name}/train_baskets.csv'\n",
    "path_test = f'data/{dataset_name}/test_baskets.csv'\n",
    "path_val = f'data/{dataset_name}/valid_baskets.csv'\n",
    "\n",
    "# path_train = f'data/dunnhumby_cj/train_baskets_sample.csv'\n",
    "# path_test = f'data/dunnhumby_cj/test_baskets.csv'\n",
    "# path_val = f'data/dunnhumby_cj/valid_baskets.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: data/dunnhumby_cj_test: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir data/dunnhumby_cj_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of test users: 981\n",
      "items: 22924\n",
      "filtered items: 5078\n"
     ]
    }
   ],
   "source": [
    "dataset = DatasetInit(\n",
    "    path_train=path_train,\n",
    "    path_val=path_val,\n",
    "    path_test=path_test,\n",
    "    dataset=dataset_name,\n",
    "    history_len=history_len,\n",
    "    basket_count_min=3,\n",
    "    min_item_count=5\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [00:00, 98.44it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num users: 981\n",
      "1 user passed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "981it [00:12, 77.32it/s] \n",
      "533it [00:00, 2322.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201 user passed\n",
      "301 user passed\n",
      "401 user passed\n",
      "601 user passed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1250it [00:00, 3289.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "901 user passed\n",
      "1001 user passed\n",
      "1101 user passed\n",
      "1201 user passed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "701it [00:00, 3551.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 user passed\n",
      "201 user passed\n",
      "401 user passed\n",
      "601 user passed\n",
      "701 user passed\n",
      "801 user passed\n",
      "901 user passed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1250it [00:00, 2812.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1001 user passed\n",
      "1101 user passed\n",
      "1201 user passed\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10000 \n",
    "\n",
    "train_dataset = CustomDatasetLarge(\n",
    "    dataset=dataset, \n",
    "    mode='train'\n",
    "    )\n",
    "# train_dataset = CustomDatasetSmall(\n",
    "#     dataset=dataset, \n",
    "#     mode='train'\n",
    "#     )\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True\n",
    "    )\n",
    "\n",
    "val_dataset = CustomDatasetSmall(\n",
    "    dataset=dataset, \n",
    "    mode='val'\n",
    "    )\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False\n",
    "    )\n",
    "\n",
    "test_dataset = CustomDatasetSmall(\n",
    "    dataset=dataset, \n",
    "    mode='test'\n",
    "    )\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = ToDevice(\n",
    "    dl=train_loader, \n",
    "    device=device\n",
    "    )\n",
    "val_loader = ToDevice(\n",
    "    dl=val_loader, \n",
    "    device=device\n",
    "    )\n",
    "test_loader = ToDevice(\n",
    "    dl=test_loader, \n",
    "    device=device\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(\n",
    "        model,\n",
    "        test_loader,\n",
    "        regime='test'\n",
    "        ):\n",
    "    \n",
    "    test_items, test_users, test_history2, test_labels = dataset.create_test_data(\n",
    "        regime\n",
    "        )\n",
    "    \n",
    "    preds = []\n",
    "    pred_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for item_input, user_input,  history_input, target in tqdm(test_loader):\n",
    "            y_pred = model(\n",
    "                item_input=item_input,\n",
    "                user_input=user_input,\n",
    "                history_input=history_input\n",
    "                )\n",
    "            \n",
    "            pred = [round(value) for value in y_pred.flatten().tolist()]\n",
    "            preds.extend(pred)\n",
    "\n",
    "            pred_scores.extend(y_pred.flatten().tolist())\n",
    "\n",
    "    prediction_baskets = {}\n",
    "    prediction_scores = {}\n",
    "    for user in tqdm(dataset.test_users):\n",
    "        top_items = []\n",
    "        if user in dataset.user_id_mapper:\n",
    "            user_id = dataset.user_id_mapper[user]\n",
    "            indices = np.argwhere(test_users == user_id)\n",
    "            item_scores = np.array(pred_scores)[indices].flatten()\n",
    "            item_ids = test_items[indices].flatten()\n",
    "            item_score_dic = {}\n",
    "            for i, item_id in enumerate(item_ids):\n",
    "                item_score_dic[dataset.id_item_mapper[item_id]] = item_scores[i]\n",
    "            sorted_item_scores = sorted(item_score_dic.items(), key=lambda x: x[1], reverse = True)\n",
    "            top_items = [x[0] for x in sorted_item_scores]\n",
    "            prediction_scores[user] = sorted_item_scores\n",
    "            \n",
    "        prediction_baskets[user] = top_items\n",
    "\n",
    "    return prediction_baskets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "        model,\n",
    "        epochs,\n",
    "        checkpoint=True,\n",
    "        wandb_name='ReCANet'\n",
    "        ):\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    wandb.init(\n",
    "        project='recanet',\n",
    "        name=wandb_name,\n",
    "        tags=[\n",
    "            'all_data',\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    wandb.define_metric('epoch_step/train')\n",
    "    wandb.define_metric('epoch/train/*', step_metric='epoch_step/train')\n",
    "    \n",
    "    wandb.define_metric('batch_step/train')\n",
    "    wandb.define_metric('train/batch/*', step_metric='batch_step/train')\n",
    "    \n",
    "    wandb.define_metric('epoch_step/valid')\n",
    "    wandb.define_metric('epoch/valid/*', step_metric='epoch_step/valid')\n",
    "    \n",
    "    wandb.define_metric('batch_step/valid')\n",
    "    wandb.define_metric('batch/valid/*', step_metric='batch_step/valid')\n",
    "    \n",
    "    wandb.define_metric('epoch_step/test')\n",
    "    wandb.define_metric('epoch/test/*', step_metric='epoch_step/test')\n",
    "    \n",
    "    loss_train = []\n",
    "    loss_val = []\n",
    "    metric_val = []\n",
    "    metric_train = []\n",
    "\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = torch.optim.AdamW(params=parameters, lr=0.001)\n",
    "    criterion = nn.BCELoss(reduction='mean')\n",
    "\n",
    "    for i in tqdm(range(epochs)):\n",
    "        print(f'Epoch {i}')\n",
    "        loss_train_epoch = []\n",
    "        loss_val_epoch = []\n",
    "        metric_val_epoch = []\n",
    "        metric_train_epoch = []\n",
    "\n",
    "        model.train()\n",
    "        wandb.watch(\n",
    "            model,\n",
    "            log='all'\n",
    "            )\n",
    "        \n",
    "        for step_id, (item_input, user_input, history_input, target) in tqdm(enumerate(train_loader)):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            y_pred = model(\n",
    "                item_input=item_input,\n",
    "                user_input=user_input,\n",
    "                history_input=history_input)\n",
    "            \n",
    "            predictions = [round(value) for value in y_pred.flatten().tolist()]\n",
    "        \n",
    "            loss = criterion(\n",
    "                y_pred,\n",
    "                target.to(device)\n",
    "                )\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_train_epoch.append(loss.item())\n",
    "            \n",
    "            metric_value = accuracy_score(\n",
    "                target.detach().cpu().numpy(),\n",
    "                predictions\n",
    "                )\n",
    "            metric_train_epoch.append(metric_value)\n",
    "            \n",
    "            wandb.log({\n",
    "                'batch/train/loss': loss.item(),\n",
    "                'batch/train/accuracy': metric_value,\n",
    "                'batch_step/train': step_id\n",
    "            })\n",
    "            \n",
    "        loss_train.append(np.mean(loss_train_epoch))\n",
    "        metric_train.append(np.mean(metric_train_epoch))\n",
    "        wandb.log({\n",
    "            'epoch_step/train': i,\n",
    "            'epoch/train/loss': np.mean(loss_train_epoch),\n",
    "            'epoch/train/accuracy': np.mean(metric_train_epoch)\n",
    "        })\n",
    "\n",
    "        model.eval()\n",
    "        full_y = []\n",
    "        full_predictions = []\n",
    "        pred_scores = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for step_id, (item_input, user_input,  history_input, target) in enumerate(val_loader):\n",
    "                y_pred = model(item_input, user_input, history_input)\n",
    "                \n",
    "                loss = criterion(y_pred, target)\n",
    "\n",
    "                predictions = [round(value) for value in y_pred.flatten().tolist()]\n",
    "\n",
    "                full_y.extend(target.detach().cpu())\n",
    "                pred_scores.extend(y_pred.detach().cpu())\n",
    "                full_predictions.extend(predictions)\n",
    "\n",
    "                loss_val_epoch.append(loss.item())\n",
    "                metric_val_epoch.append(accuracy_score(target.cpu(), predictions))\n",
    "                \n",
    "                wandb.log({\n",
    "                    'batch/valid/loss': loss.item(),\n",
    "                    'batch/valid/accuracy': accuracy_score(target.cpu(), predictions),\n",
    "                    'batch_step/valid': step_id\n",
    "                })\n",
    "                \n",
    "        accuracy = accuracy_score(full_y, full_predictions)\n",
    "        print(\"Accuracy epoch: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "        val_baskets = pd.read_csv(path_val)\n",
    "        user_val_baskets_df = val_baskets.groupby('user_id')['item_id'].apply(list).reset_index()\n",
    "        user_val_baskets_dict = dict(zip(user_val_baskets_df['user_id'], user_val_baskets_df['item_id']))\n",
    "\n",
    "        user_predictions = predict(model, val_loader, 'val')\n",
    "        final_users = set(dataset.test_users).intersection(set(list(user_val_baskets_dict.keys())))\n",
    "        print('predictions ready', len(user_predictions))\n",
    "        print('number of final test users:', len(final_users))\n",
    "        log_dict_valid = {}\n",
    "        for k in [5, 10, 20, 'B']:\n",
    "            print(k)\n",
    "            recall_scores = {}\n",
    "            ndcg_scores = {}\n",
    "            for user in final_users:\n",
    "\n",
    "                top_items = []\n",
    "                if user in user_predictions:\n",
    "                    top_items = user_predictions[user]\n",
    "\n",
    "                if k == 'B':\n",
    "                    recall_scores[user] = recall_k(\n",
    "                        y_true=user_val_baskets_dict[user],\n",
    "                        y_pred=top_items,\n",
    "                        k=len(user_val_baskets_dict[user])\n",
    "                        )\n",
    "                    ndcg_scores[user] = ndcg_k(\n",
    "                        y_true=user_val_baskets_dict[user],\n",
    "                        y_pred=top_items,\n",
    "                        k=len(user_val_baskets_dict[user])\n",
    "                        )\n",
    "                else:\n",
    "                    recall_scores[user] = recall_k(\n",
    "                        y_true=user_val_baskets_dict[user],\n",
    "                        y_pred=top_items,\n",
    "                        k=k\n",
    "                        )\n",
    "                    ndcg_scores[user] = ndcg_k(\n",
    "                        y_true=user_val_baskets_dict[user],\n",
    "                        y_pred=top_items,\n",
    "                        k=k\n",
    "                        )\n",
    "            print('recall:', np.mean(list(recall_scores.values())))\n",
    "            print('ndcg:', np.mean(list(ndcg_scores.values())))\n",
    "            log_dict_valid[f'epoch/valid/recall_{k}'] = np.mean(list(recall_scores.values()))\n",
    "            log_dict_valid[f'epoch/valid/ndcg_{k}'] = np.mean(list(ndcg_scores.values()))\n",
    "\n",
    "        wandb.log({\n",
    "            **{\n",
    "                'epoch_step/valid': i,\n",
    "                'epoch/valid/accuracy': np.mean(metric_val_epoch),\n",
    "                'epoch/valid/loss': np.mean(loss_val_epoch)\n",
    "            },\n",
    "            **log_dict_valid,\n",
    "            })\n",
    "\n",
    "        loss_val.append(np.mean(loss_val_epoch))\n",
    "        metric_val.append(np.mean(metric_val_epoch))\n",
    "        \n",
    "        test_baskets = pd.read_csv(path_test)\n",
    "        user_test_baskets_df = test_baskets.groupby('user_id')['item_id'].apply(list).reset_index()\n",
    "        user_test_baskets_dict = dict(zip(user_test_baskets_df['user_id'], user_test_baskets_df['item_id']))\n",
    "\n",
    "        user_predictions = predict(model, test_loader, 'test')\n",
    "        final_users = set(dataset.test_users).intersection(set(list(user_test_baskets_dict.keys())))\n",
    "        print('predictions ready', len(user_predictions))\n",
    "        print('number of final test users:', len(final_users))\n",
    "        log_dict_test = {}\n",
    "        for k in [5, 10, 20, 'B']:\n",
    "            print(k)\n",
    "            recall_scores = {}\n",
    "            ndcg_scores = {}\n",
    "            for user in final_users:\n",
    "\n",
    "                top_items = []\n",
    "                if user in user_predictions:\n",
    "                    top_items = user_predictions[user]\n",
    "\n",
    "                if k == 'B':\n",
    "                    recall_scores[user] = recall_k(\n",
    "                        y_true=user_test_baskets_dict[user],\n",
    "                        y_pred=top_items,\n",
    "                        k=len(user_test_baskets_dict[user])\n",
    "                        )\n",
    "                    ndcg_scores[user] = ndcg_k(\n",
    "                        y_true=user_test_baskets_dict[user],\n",
    "                        y_pred=top_items,\n",
    "                        k=len(user_test_baskets_dict[user]))\n",
    "                else:\n",
    "                    recall_scores[user] = recall_k(\n",
    "                        y_true=user_test_baskets_dict[user],\n",
    "                        y_pred=top_items,\n",
    "                        k=k)\n",
    "                    ndcg_scores[user] = ndcg_k(\n",
    "                        y_true=user_test_baskets_dict[user],\n",
    "                        y_pred=top_items,\n",
    "                        k=k\n",
    "                        )\n",
    "                    # \n",
    "            print('recall:', np.mean(list(recall_scores.values())))\n",
    "            print('ndcg:', np.mean(list(ndcg_scores.values())))\n",
    "            log_dict_test[f'epoch/test/recall_{k}'] = np.mean(list(recall_scores.values()))\n",
    "            log_dict_test[f'epoch/test/ndcg_{k}'] = np.mean(list(ndcg_scores.values()))\n",
    "        \n",
    "        wandb.log({\n",
    "            **{\n",
    "                'epoch_step/test': i,   \n",
    "            },\n",
    "            **log_dict_test,\n",
    "            })\n",
    "        \n",
    "        if checkpoint:\n",
    "            path = f'wallykop_epoch{i}_recanet_dunnhumby_20.pth'\n",
    "            torch.save(model.state_dict(), path)\n",
    "            np.save('loss_train.npy', loss_train)\n",
    "            np.save('loss_val.npy', loss_val)\n",
    "            np.save('metric_train.npy', metric_train)\n",
    "            np.save('metric_val.npy', metric_val)\n",
    "            \n",
    "            wandb.save(path)\n",
    "        \n",
    "    wandb.finish()\n",
    "    return loss_train, loss_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.reCANet_base import ReCaNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.reCANet_Attention import ReCaNet_Attention, Bidir_ReCaNet_Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.reCANet_Pooling import ReCaNet_Pooling\n",
    "from models.pooling_layers import AttentionPooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.reCANet_MHSA import ReCaNet_separate_MHSA, ReCaNet_user_item_MHSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.reCANet_Transformer import ReCaNet_Transformer, ReCaNet_MHSA_Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# poolings2 = [AttentionPooling(lstm_hidden_size)]\n",
    "# model = ReCaNet_MHSA_Transformer(\n",
    "#     num_items=dataset.num_items, \n",
    "#     item_embed_size=item_embed_size, \n",
    "#     num_users=dataset.num_users, \n",
    "#     user_embed_size=user_embed_size, \n",
    "#     history_len = history_len, \n",
    "#     user_item_hidden_size = user_item_hidden_size,\n",
    "#     user_item_history_hidden_size = user_item_history_hidden_size, \n",
    "#     lstm_hidden_size = lstm_hidden_size,\n",
    "#     dense_1_hidden_size = dense_1_hidden_size, \n",
    "#     dense_2_hidden_size = dense_2_hidden_size,\n",
    "#     # poolings=poolings2\n",
    "#     ).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ReCaNet(\n",
    "    num_items=dataset.num_items, \n",
    "    item_embed_size=item_embed_size, \n",
    "    num_users=dataset.num_users, \n",
    "    user_embed_size=user_embed_size, \n",
    "    history_len = history_len, \n",
    "    user_item_hidden_size = user_item_hidden_size,\n",
    "    user_item_history_hidden_size = user_item_history_hidden_size, \n",
    "    lstm_hidden_size = lstm_hidden_size,\n",
    "    dense_1_hidden_size = dense_1_hidden_size, \n",
    "    dense_2_hidden_size = dense_2_hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ReCaNet_MHSA_Transformer(\n",
    "    num_items=dataset.num_items, \n",
    "    item_embed_size=item_embed_size, \n",
    "    num_users=dataset.num_users, \n",
    "    user_embed_size=user_embed_size, \n",
    "    history_len = history_len, \n",
    "    user_item_hidden_size = user_item_hidden_size,\n",
    "    user_item_history_hidden_size = user_item_history_hidden_size, \n",
    "    lstm_hidden_size = lstm_hidden_size,\n",
    "    dense_1_hidden_size = dense_1_hidden_size, \n",
    "    dense_2_hidden_size = dense_2_hidden_size,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ReCaNet_Transformer(\n",
    "    num_items=dataset.num_items, \n",
    "    item_embed_size=item_embed_size, \n",
    "    num_users=dataset.num_users, \n",
    "    user_embed_size=user_embed_size, \n",
    "    history_len = history_len, \n",
    "    user_item_hidden_size = user_item_hidden_size,\n",
    "    user_item_history_hidden_size = user_item_history_hidden_size, \n",
    "    lstm_hidden_size = lstm_hidden_size,\n",
    "    dense_1_hidden_size = dense_1_hidden_size, \n",
    "    dense_2_hidden_size = dense_2_hidden_size,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ReCaNet_separate_MHSA(\n",
    "    num_items=dataset.num_items, \n",
    "    item_embed_size=item_embed_size, \n",
    "    num_users=dataset.num_users, \n",
    "    user_embed_size=user_embed_size, \n",
    "    history_len = history_len, \n",
    "    user_item_hidden_size = user_item_hidden_size,\n",
    "    user_item_history_hidden_size = user_item_history_hidden_size, \n",
    "    lstm_hidden_size = lstm_hidden_size,\n",
    "    dense_1_hidden_size = dense_1_hidden_size, \n",
    "    dense_2_hidden_size = dense_2_hidden_size,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ReCaNet_user_item_MHSA(\n",
    "    num_items=dataset.num_items, \n",
    "    item_embed_size=item_embed_size, \n",
    "    num_users=dataset.num_users, \n",
    "    user_embed_size=user_embed_size, \n",
    "    history_len = history_len, \n",
    "    user_item_hidden_size = user_item_hidden_size,\n",
    "    user_item_history_hidden_size = user_item_history_hidden_size, \n",
    "    lstm_hidden_size = lstm_hidden_size,\n",
    "    dense_1_hidden_size = dense_1_hidden_size, \n",
    "    dense_2_hidden_size = dense_2_hidden_size,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poolings2 = [AttentionPooling(lstm_hidden_size)]\n",
    "model = ReCaNet_Pooling(\n",
    "    num_items=dataset.num_items, \n",
    "    item_embed_size=item_embed_size, \n",
    "    num_users=dataset.num_users, \n",
    "    user_embed_size=user_embed_size, \n",
    "    history_len = history_len, \n",
    "    user_item_hidden_size = user_item_hidden_size,\n",
    "    user_item_history_hidden_size = user_item_history_hidden_size, \n",
    "    lstm_hidden_size = lstm_hidden_size,\n",
    "    dense_1_hidden_size = dense_1_hidden_size, \n",
    "    dense_2_hidden_size = dense_2_hidden_size,\n",
    "    poolings=poolings2\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Bidir_ReCaNet_Attention(\n",
    "    num_items=dataset.num_items, \n",
    "    item_embed_size=item_embed_size, \n",
    "    num_users=dataset.num_users, \n",
    "    user_embed_size=user_embed_size, \n",
    "    history_len = history_len, \n",
    "    user_item_hidden_size = user_item_hidden_size,\n",
    "    user_item_history_hidden_size = user_item_history_hidden_size, \n",
    "    lstm_hidden_size = lstm_hidden_size,\n",
    "    dense_1_hidden_size = dense_1_hidden_size, \n",
    "    dense_2_hidden_size = dense_2_hidden_size,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train, loss_val = train(\n",
    "    model=model, \n",
    "    epochs=5, \n",
    "    checkpoint=True, \n",
    "    wandb_name='ReCANet'\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bki_nn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
